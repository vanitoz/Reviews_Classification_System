{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through a necessary step of any data science project - data cleaning. Data cleaning is very important. Keep in mind, \"garbage in, garbage out\". Feeding dirty data into a model will give us results that are meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data from Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import sqlite3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpTokenizer, PorterStemmer, WordNetLemmatizer, FreqDist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from utils import * # import all existing functions from file utils.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../data/business_places.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alan H.</td>\n",
       "      <td>Tampa, FL</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3/2/2020</td>\n",
       "      <td>KvLrI20Abz6F9WzAIHmyWQ</td>\n",
       "      <td>With Cafe R and the hotel having a deal for us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shraddha R.</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Negative</td>\n",
       "      <td>5/4/2019</td>\n",
       "      <td>KvLrI20Abz6F9WzAIHmyWQ</td>\n",
       "      <td>Went here because we had a food coupon we didn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michelle L.</td>\n",
       "      <td>Washington Heights, Manhattan, NY</td>\n",
       "      <td>Negative</td>\n",
       "      <td>7/21/2020</td>\n",
       "      <td>KvLrI20Abz6F9WzAIHmyWQ</td>\n",
       "      <td>As you guys are aware majority of places are c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paul L.</td>\n",
       "      <td>North Babylon, NY</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1/26/2020</td>\n",
       "      <td>KvLrI20Abz6F9WzAIHmyWQ</td>\n",
       "      <td>Went here fo continental breakfast and this pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andrew G.</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Positive</td>\n",
       "      <td>11/27/2019</td>\n",
       "      <td>KvLrI20Abz6F9WzAIHmyWQ</td>\n",
       "      <td>Great cafe located in a busy section of Chelse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name                           location    rating        date  \\\n",
       "0      Alan H.                          Tampa, FL   Neutral    3/2/2020   \n",
       "1  Shraddha R.                       New York, NY  Negative    5/4/2019   \n",
       "2  Michelle L.  Washington Heights, Manhattan, NY  Negative   7/21/2020   \n",
       "3      Paul L.                  North Babylon, NY  Negative   1/26/2020   \n",
       "4    Andrew G.                      Manhattan, NY  Positive  11/27/2019   \n",
       "\n",
       "              business_id                                               text  \n",
       "0  KvLrI20Abz6F9WzAIHmyWQ  With Cafe R and the hotel having a deal for us...  \n",
       "1  KvLrI20Abz6F9WzAIHmyWQ  Went here because we had a food coupon we didn...  \n",
       "2  KvLrI20Abz6F9WzAIHmyWQ  As you guys are aware majority of places are c...  \n",
       "3  KvLrI20Abz6F9WzAIHmyWQ  Went here fo continental breakfast and this pl...  \n",
       "4  KvLrI20Abz6F9WzAIHmyWQ  Great cafe located in a busy section of Chelse...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(cursor.execute(\"SELECT * FROM corpus\").fetchall())\n",
    "data.columns = [x[0] for x in cursor.description]\n",
    "data = data.drop(columns = ['index'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37907 entries, 0 to 37906\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         37907 non-null  object\n",
      " 1   location     37907 non-null  object\n",
      " 2   rating       37907 non-null  object\n",
      " 3   date         37907 non-null  object\n",
      " 4   business_id  37907 non-null  object\n",
      " 5   text         37907 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data\n",
    "\n",
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common data cleaning steps on all text:\n",
    "\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More data cleaning steps after tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting corpus First\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.loc[:, ['text','location','rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets clean text of each review with function cleaning_text from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[:,'clean_text'] = corpus.loc[:,'text'].apply(cleaning_text)\n",
    "\n",
    "# Words with 1-2 letters that can be usless we can use filter_review function to clean this words\n",
    "corpus.loc[:,'clean_text'] = corpus.loc[:, 'clean_text'].apply(filter_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tokenization \n",
    "Tokenization is the process of splitting documents into units of observations. We usually represent the tokens as __n-gram__, where n represent the consecutive words occuring in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[:,'token_text'] = corpus.loc[:, 'clean_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming \n",
    "Stemming allows us to remove different variations of the same word. For example, collect, collection and collecting will all be reduced to the same single word collect.\n",
    "- Stemming is the process of reducing inflection in words to their root forms, such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
    "- Stems are created by removing the suffixes or prefixes used with a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "The only difference between lemmatization and stemming is that lemmatization returns real words. For example, instead of returning \"movi\" like Porter stemmer would, \"movie\" will be returned by the lemmatizer.\n",
    "\n",
    "- Unlike Stemming, Lemmatization reduces the inflected words properly ensuring that the root word belongs to the language. \n",
    "\n",
    "- In Lemmatization, the root word is called Lemma. \n",
    "\n",
    "- A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "corpus['lem'] = corpus['token_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "corpus['stem'] = corpus['token_text'].apply(lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create syntactic features\n",
    "\n",
    "Using NLTK to create context-free grammar and part-of-speech (POS) tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['pos']  = corpus['token_text'].apply(lambda x: [pos_tag[1] for pos_tag in pos_tag(x)])    # bag of tags\n",
    "\n",
    "# calculate percentage of nouns in each review\n",
    "corpus['noun'] = corpus['pos'].apply(lambda x: sum(1 for pos in x if pos.startswith('NN')) / len(x) if len(x) > 0 else 0) \n",
    "\n",
    "# calculate percentage of adjectives in each review\n",
    "corpus['adj']  = corpus['pos'].apply(lambda x: sum(1 for pos in x if pos.startswith('JJ')) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "# calculate percentage of adverbs in each review\n",
    "corpus['adv'] = corpus['pos'].apply(lambda x: sum(1 for pos in x if pos.startswith('RB')) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "# calculate percentage of verbs in each review\n",
    "corpus['verb'] = corpus['pos'].apply(lambda x: sum(1 for pos in x if pos.startswith('VB')) / len(x) if len(x) > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Structual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['characters'] = corpus['text'].apply(lambda x: len(x))            # number of characters\n",
    "corpus['tokens'] = corpus['token_text'].apply(lambda x: len(x))          # number of tokens\n",
    "corpus['words'] = corpus['text'].apply(lambda x: len(x.split(' ')))      # number of words\n",
    "corpus['sentences'] = corpus['text'].apply(lambda x: len(x.split('. '))) # number of sentences\n",
    "corpus['avg_word_len'] = corpus['characters'] / corpus['words']          # average word length\n",
    "corpus['avg_sent_len'] = corpus['words'] / corpus['sentences']           # average sentence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location specify home place of the person who left review. We can turn this column into categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[:, 'location'] = corpus.loc[:, 'location'].map(lambda x : 1 if 'NY' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37907 entries, 0 to 37906\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   text          37907 non-null  object \n",
      " 1   location      37907 non-null  int64  \n",
      " 2   rating        37907 non-null  object \n",
      " 3   clean_text    37907 non-null  object \n",
      " 4   token_text    37907 non-null  object \n",
      " 5   lem           37907 non-null  object \n",
      " 6   stem          37907 non-null  object \n",
      " 7   pos           37907 non-null  object \n",
      " 8   noun          37907 non-null  float64\n",
      " 9   adj           37907 non-null  float64\n",
      " 10  adv           37907 non-null  float64\n",
      " 11  verb          37907 non-null  float64\n",
      " 12  characters    37907 non-null  int64  \n",
      " 13  tokens        37907 non-null  int64  \n",
      " 14  words         37907 non-null  int64  \n",
      " 15  sentences     37907 non-null  int64  \n",
      " 16  avg_word_len  37907 non-null  float64\n",
      " 17  avg_sent_len  37907 non-null  float64\n",
      "dtypes: float64(6), int64(5), object(7)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving cleaned corpus to DB and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv('../data/corpus_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.drop(columns = 'pos')\n",
    "\n",
    "corpus['lem'] = corpus['lem'].apply(lambda x : ' '.join(x))\n",
    "corpus['stem'] = corpus['stem'].apply(lambda x : ' '.join(x))\n",
    "corpus['token_text'] = corpus['token_text'].apply(lambda x : ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7fb4155688f0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('DROP TABLE IF EXISTS {};'.format('corpus_processed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_sql('corpus_processed', con= conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
